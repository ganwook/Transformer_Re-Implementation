{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import temp\n",
    "import model as trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = temp.get_some_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,    14,    28,    15,    36,    12,    95,   455,     9,   143,\n",
       "          119,     8,  1293,    12, 18593,   134,   101,    10,   158,    80,\n",
       "          103,  3658,     9,  2432,    47, 18703,     7, 15800,     3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b.unsqueeze(-2) #batch로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = temp.emb_pe(b, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 29, 128])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_ = trf.Generator(128, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = softmax_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 40000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "from embedding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_src_in_batch = 40\n",
    "max_tgt_in_batch = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, src, trg=None, pad=1):\n",
    "        self.src = src\n",
    "        self.src_mask = self.make_std_mask(self.src, pad)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]  # input으로 사용할때\n",
    "            self.trg_y = trg[:, 1:] # 최종 정답으로 사용할때\n",
    "            self.trg_mask = self.make_sub_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_sub_mask(tgt, pad=1):\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            trf.subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(src, pad=1):\n",
    "        src_mask = (src != pad).unsqueeze(-2)\n",
    "        src_mask = src_mask & Variable(\n",
    "            torch.ones(src.size(1), src.size(1)).type_as(src_mask.data)) \\\n",
    "        & src_mask.transpose(-2,-1)\n",
    "        return src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 11\n",
    "dummy = data_gen(11, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(dummy):\n",
    "    batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = batch.src\n",
    "trg = batch.trg\n",
    "trg_y = batch.trg_y\n",
    "src_mask = batch.src_mask\n",
    "trg_mask = batch.trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 1\n",
    "src_mask = (src != pad).unsqueeze(-2).type_as(src.data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 10])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7524e+04, -1.1778e+03,  3.1454e+02, -1.7570e+03, -1.8875e+01,\n",
       "           4.2389e+02, -4.5163e+02, -2.3499e+02, -9.8924e+02, -9.5176e+02,\n",
       "          -1.3709e+03,  2.9138e+02,  2.4241e+02,  4.2537e+01,  1.5596e+03,\n",
       "           2.2042e+03,  8.2435e+02, -6.2846e+02,  4.9981e+01,  2.8363e+02,\n",
       "           3.0681e+02, -2.0295e+02, -2.6295e+03, -5.5517e+02, -2.0805e+03,\n",
       "           7.7382e+02, -2.5679e+03,  1.3081e+03,  1.0738e+03],\n",
       "         [-1.1778e+03,  1.4555e+04, -1.5120e+03, -8.1292e+02, -2.9660e+03,\n",
       "           8.9928e+02, -3.4906e+03,  4.4633e+02, -3.0438e+03, -8.6248e+02,\n",
       "          -3.3758e+02,  6.9399e+01,  1.2673e+03,  3.9680e+02,  1.2422e+03,\n",
       "          -2.0354e+03, -2.1544e+03,  7.8515e+01, -2.5882e+03, -2.0036e+03,\n",
       "          -1.0349e+03,  1.5113e+03, -3.1160e+03,  3.4759e+02,  1.4447e+02,\n",
       "          -6.9577e+02,  3.5558e+02,  1.7925e+03, -1.6343e+02],\n",
       "         [ 3.1454e+02, -1.5120e+03,  1.8824e+04,  7.5739e+02,  2.3973e+03,\n",
       "          -7.8023e+02,  3.2932e+03,  1.2247e+03,  1.6095e+03,  5.9119e+02,\n",
       "           1.0407e+03, -1.7256e+03, -5.8289e+02, -1.1061e+02, -1.4631e+03,\n",
       "           6.7578e+02,  3.2311e+03, -1.0626e+03,  5.2049e+02,  2.4140e+03,\n",
       "          -1.2994e+03, -1.3240e+03,  9.5663e+02, -3.4430e+02,  2.5358e+01,\n",
       "          -1.0452e+03, -1.8335e+03,  2.5613e+03,  2.1758e+03],\n",
       "         [-1.7570e+03, -8.1292e+02,  7.5739e+02,  1.4430e+04, -4.0651e-01,\n",
       "           1.2942e+03, -1.1259e+03, -7.8098e+02, -1.5200e+03,  2.2799e+03,\n",
       "          -7.3881e+02, -1.7504e+03, -2.3994e+03,  1.7865e+02,  1.3506e+03,\n",
       "           7.3847e+02,  2.6707e+03,  1.7914e+03, -9.3960e+02,  1.1193e+03,\n",
       "           1.5110e+03,  3.7875e+02, -1.7119e+03,  1.2204e+03, -1.0085e+03,\n",
       "          -6.6735e+01, -2.9244e+02,  3.1726e+02, -9.2062e+01],\n",
       "         [-1.8875e+01, -2.9660e+03,  2.3973e+03, -4.0651e-01,  2.1142e+04,\n",
       "          -4.5752e+02,  2.6403e+03,  2.8857e+02,  2.4689e+03,  2.4505e+03,\n",
       "          -1.1303e+03, -1.5513e+03, -5.8835e+02, -1.0205e+03, -9.3667e+02,\n",
       "           2.7292e+02, -1.1987e+03,  5.7632e+02, -1.3757e+03, -3.3269e+03,\n",
       "          -1.3301e+03,  2.8753e+03,  3.3404e+03, -1.6860e+03, -1.9370e+03,\n",
       "           2.2522e+03, -1.3631e+03,  2.6044e+03,  3.7681e+03],\n",
       "         [ 4.2389e+02,  8.9928e+02, -7.8023e+02,  1.2942e+03, -4.5752e+02,\n",
       "           1.6689e+04, -1.2057e+03,  1.9509e+03, -1.8631e+02, -2.3125e+03,\n",
       "           1.2733e+03, -2.0632e+03,  1.8473e+03,  1.5310e+04,  2.3449e+03,\n",
       "          -2.3566e+03,  2.3006e+03,  6.4362e+02, -1.4071e+03,  3.5399e+02,\n",
       "           4.3717e+03,  1.7081e+03, -1.0783e+03, -8.3835e+02, -2.0323e+03,\n",
       "          -4.7488e+02, -2.2258e+02,  8.3697e+01, -3.5643e+02],\n",
       "         [-4.5163e+02, -3.4906e+03,  3.2932e+03, -1.1259e+03,  2.6403e+03,\n",
       "          -1.2057e+03,  2.3662e+04,  6.7711e+02, -1.5293e+03,  2.5065e+03,\n",
       "          -1.4964e+03, -1.6344e+02, -2.1604e+03, -1.3301e+03, -2.6578e+03,\n",
       "          -3.2781e+01,  4.3642e+03, -2.0877e+03,  9.4889e+02, -2.0322e+03,\n",
       "          -1.3231e+03, -2.1668e+03, -6.2538e+02, -6.5034e+02, -2.0825e+03,\n",
       "          -1.9667e+03, -6.3713e+02, -1.2485e+03,  7.6695e+02],\n",
       "         [-2.3499e+02,  4.4633e+02,  1.2247e+03, -7.8098e+02,  2.8857e+02,\n",
       "           1.9509e+03,  6.7711e+02,  1.7141e+04, -1.0616e+03, -1.7770e+03,\n",
       "           2.9636e+03, -7.5624e+02,  1.0488e+03,  2.0526e+03,  1.1764e+03,\n",
       "           2.0152e+02,  2.0994e+03, -1.0490e+03, -6.7046e+02, -1.8475e+03,\n",
       "          -1.0067e+03, -1.3265e+03, -1.1306e+03,  9.5285e+02,  2.4059e+03,\n",
       "           1.4797e+01,  3.2927e+01, -1.9296e+03,  3.5134e+02],\n",
       "         [-9.8924e+02, -3.0438e+03,  1.6095e+03, -1.5200e+03,  2.4689e+03,\n",
       "          -1.8631e+02, -1.5293e+03, -1.0616e+03,  1.7459e+04,  1.0624e+03,\n",
       "          -4.0099e+02,  2.6043e+02,  3.7645e+02, -6.3106e+02, -1.6079e+03,\n",
       "          -4.4668e+02, -8.6037e+02,  1.1206e+03, -8.5785e+02,  1.4577e+03,\n",
       "          -2.0732e+03,  4.0331e+03,  1.5815e+04,  2.3980e+02, -6.8619e+02,\n",
       "          -1.5139e+03,  4.3952e+02,  4.3524e+02,  3.2218e+02],\n",
       "         [-9.5176e+02, -8.6248e+02,  5.9119e+02,  2.2799e+03,  2.4505e+03,\n",
       "          -2.3125e+03,  2.5065e+03, -1.7770e+03,  1.0624e+03,  2.1366e+04,\n",
       "          -4.2871e+02, -3.7549e+02, -2.5305e+03, -2.8570e+03, -2.0420e+03,\n",
       "          -2.6984e+02,  1.1588e+03,  2.9627e+03, -1.6906e+03, -5.1411e+02,\n",
       "           4.0071e+02, -1.0224e+03,  3.3145e+03, -1.1831e+03, -1.3833e+03,\n",
       "          -2.0284e+03, -2.3081e+02, -8.7431e+02, -7.2299e+02],\n",
       "         [-1.3709e+03, -3.3758e+02,  1.0407e+03, -7.3881e+02, -1.1303e+03,\n",
       "           1.2733e+03, -1.4964e+03,  2.9636e+03, -4.0099e+02, -4.2871e+02,\n",
       "           1.5204e+04,  1.2355e+03, -1.7605e+02,  1.3429e+03,  1.7466e+03,\n",
       "          -6.5718e+02, -1.8207e+02,  2.2578e+03, -5.6438e+02,  2.3664e+03,\n",
       "          -2.7293e+02,  1.2544e+03,  4.0780e+02,  2.6117e+03,  1.9665e+03,\n",
       "          -1.2302e+02,  9.3756e+02,  1.0513e+03, -1.2597e+03],\n",
       "         [ 2.9138e+02,  6.9399e+01, -1.7256e+03, -1.7504e+03, -1.5513e+03,\n",
       "          -2.0632e+03, -1.6344e+02, -7.5624e+02,  2.6043e+02, -3.7549e+02,\n",
       "           1.2355e+03,  1.7678e+04, -2.3893e+02, -1.6710e+03, -1.0694e+03,\n",
       "          -1.3436e+03,  5.9791e+02, -2.1050e+03,  1.4990e+03,  1.8741e+03,\n",
       "           1.9605e+03, -1.5365e+03,  3.4055e+02,  1.0546e+03,  9.4133e+01,\n",
       "          -1.1926e+03, -2.5111e+03, -5.9279e+02, -2.0614e+03],\n",
       "         [ 2.4241e+02,  1.2673e+03, -5.8289e+02, -2.3994e+03, -5.8835e+02,\n",
       "           1.8473e+03, -2.1604e+03,  1.0488e+03,  3.7645e+02, -2.5305e+03,\n",
       "          -1.7605e+02, -2.3893e+02,  1.5015e+04,  1.6840e+03,  1.4234e+03,\n",
       "          -1.3964e+03,  1.1952e+03, -4.0000e+02, -1.1206e+03, -1.9127e+03,\n",
       "           7.6206e+02,  1.7121e+03,  2.0370e+02,  1.9107e+03, -2.7515e+03,\n",
       "           1.0704e+03, -1.3023e+03, -1.2551e+03,  7.0670e+02],\n",
       "         [ 4.2537e+01,  3.9680e+02, -1.1061e+02,  1.7865e+02, -1.0205e+03,\n",
       "           1.5310e+04, -1.3301e+03,  2.0526e+03, -6.3106e+02, -2.8570e+03,\n",
       "           1.3429e+03, -1.6710e+03,  1.6840e+03,  1.7128e+04,  1.1943e+03,\n",
       "          -3.2284e+03,  2.5947e+03,  3.0410e+02, -9.6910e+02,  1.2546e+03,\n",
       "           5.2967e+03, -4.7345e+01, -1.3600e+03, -2.3264e+03, -2.5379e+03,\n",
       "           1.6996e+02,  1.4013e+02, -6.2864e+02, -2.8938e+02],\n",
       "         [ 1.5596e+03,  1.2422e+03, -1.4631e+03,  1.3506e+03, -9.3667e+02,\n",
       "           2.3449e+03, -2.6578e+03,  1.1764e+03, -1.6079e+03, -2.0420e+03,\n",
       "           1.7466e+03, -1.0694e+03,  1.4234e+03,  1.1943e+03,  2.0116e+04,\n",
       "          -1.2247e+03,  3.3996e+03, -2.6690e+03, -3.7329e+03,  1.1518e+02,\n",
       "           1.6471e+03, -1.2902e+03, -9.0462e+02,  1.8161e+03, -1.7723e+03,\n",
       "           1.8360e+03, -2.1066e+02, -6.9618e+02,  1.8985e+03],\n",
       "         [ 2.2042e+03, -2.0354e+03,  6.7578e+02,  7.3847e+02,  2.7292e+02,\n",
       "          -2.3566e+03, -3.2781e+01,  2.0152e+02, -4.4668e+02, -2.6984e+02,\n",
       "          -6.5718e+02, -1.3436e+03, -1.3964e+03, -3.2284e+03, -1.2247e+03,\n",
       "           1.7150e+04, -7.3718e+01,  1.7444e+03, -1.5856e+03,  7.7180e+02,\n",
       "           4.7527e+02, -5.2843e+02, -6.8196e+02, -7.4079e+02,  5.7279e+02,\n",
       "          -6.4991e+02, -3.4611e+03,  3.5934e+02, -2.2871e+02],\n",
       "         [ 8.2435e+02, -2.1544e+03,  3.2311e+03,  2.6707e+03, -1.1987e+03,\n",
       "           2.3006e+03,  4.3642e+03,  2.0994e+03, -8.6037e+02,  1.1588e+03,\n",
       "          -1.8207e+02,  5.9791e+02,  1.1952e+03,  2.5947e+03,  3.3996e+03,\n",
       "          -7.3718e+01,  1.6537e+04, -7.2576e+02, -6.9931e+02,  4.8780e+02,\n",
       "           3.6993e+03,  7.6486e+02, -1.3267e+03, -1.7436e+02, -2.6174e+03,\n",
       "          -8.2590e+02, -2.7114e+02, -2.1364e+03,  1.2233e+03],\n",
       "         [-6.2846e+02,  7.8515e+01, -1.0626e+03,  1.7914e+03,  5.7632e+02,\n",
       "           6.4362e+02, -2.0877e+03, -1.0490e+03,  1.1206e+03,  2.9627e+03,\n",
       "           2.2578e+03, -2.1050e+03, -4.0000e+02,  3.0410e+02, -2.6690e+03,\n",
       "           1.7444e+03, -7.2576e+02,  2.2900e+04,  3.4780e+03, -1.6384e+03,\n",
       "          -5.7022e+02,  2.3462e+03,  2.6587e+03, -3.9436e+03, -2.6426e+03,\n",
       "          -1.4723e+02,  2.3790e+03, -2.5793e+03, -1.7361e+03],\n",
       "         [ 4.9981e+01, -2.5882e+03,  5.2049e+02, -9.3960e+02, -1.3757e+03,\n",
       "          -1.4071e+03,  9.4889e+02, -6.7046e+02, -8.5785e+02, -1.6906e+03,\n",
       "          -5.6438e+02,  1.4990e+03, -1.1206e+03, -9.6910e+02, -3.7329e+03,\n",
       "          -1.5856e+03, -6.9931e+02,  3.4780e+03,  1.8128e+04, -3.8418e+02,\n",
       "           9.4789e+02, -3.3094e+02,  7.0631e+02, -1.5818e+03, -4.4347e+02,\n",
       "          -3.6250e+03,  1.6852e+03, -2.1582e+03, -1.0356e+03],\n",
       "         [ 2.8363e+02, -2.0036e+03,  2.4140e+03,  1.1193e+03, -3.3269e+03,\n",
       "           3.5399e+02, -2.0322e+03, -1.8475e+03,  1.4577e+03, -5.1411e+02,\n",
       "           2.3664e+03,  1.8741e+03, -1.9127e+03,  1.2546e+03,  1.1518e+02,\n",
       "           7.7180e+02,  4.8780e+02, -1.6384e+03, -3.8418e+02,  1.9665e+04,\n",
       "           3.0152e+03, -3.7723e+03,  8.6751e+02,  2.6284e+03,  4.9825e+02,\n",
       "          -5.8775e+02,  7.3690e+02,  8.8308e+02,  3.0033e+02],\n",
       "         [ 3.0681e+02, -1.0349e+03, -1.2994e+03,  1.5110e+03, -1.3301e+03,\n",
       "           4.3717e+03, -1.3231e+03, -1.0067e+03, -2.0732e+03,  4.0071e+02,\n",
       "          -2.7293e+02,  1.9605e+03,  7.6206e+02,  5.2967e+03,  1.6471e+03,\n",
       "           4.7527e+02,  3.6993e+03, -5.7022e+02,  9.4789e+02,  3.0152e+03,\n",
       "           2.2245e+04, -2.2733e+03, -3.3075e+03, -1.3859e+03, -7.3838e+02,\n",
       "          -6.2786e+01, -1.0046e+03,  8.6265e+02, -3.1488e+02],\n",
       "         [-2.0295e+02,  1.5113e+03, -1.3240e+03,  3.7875e+02,  2.8753e+03,\n",
       "           1.7081e+03, -2.1668e+03, -1.3265e+03,  4.0331e+03, -1.0224e+03,\n",
       "           1.2544e+03, -1.5365e+03,  1.7121e+03, -4.7345e+01, -1.2902e+03,\n",
       "          -5.2843e+02,  7.6486e+02,  2.3462e+03, -3.3094e+02, -3.7723e+03,\n",
       "          -2.2733e+03,  1.9654e+04,  3.3400e+03,  1.2114e+03, -8.2472e+01,\n",
       "           1.0761e+03, -1.6858e+03, -1.0053e+03,  1.4787e+03],\n",
       "         [-2.6295e+03, -3.1160e+03,  9.5663e+02, -1.7119e+03,  3.3404e+03,\n",
       "          -1.0783e+03, -6.2538e+02, -1.1306e+03,  1.5815e+04,  3.3145e+03,\n",
       "           4.0780e+02,  3.4055e+02,  2.0370e+02, -1.3600e+03, -9.0462e+02,\n",
       "          -6.8196e+02, -1.3267e+03,  2.6587e+03,  7.0631e+02,  8.6751e+02,\n",
       "          -3.3075e+03,  3.3400e+03,  2.0060e+04, -1.4883e+03,  2.7691e+02,\n",
       "          -2.1035e+03, -7.9128e+02, -2.5361e+02, -4.4925e+01],\n",
       "         [-5.5517e+02,  3.4759e+02, -3.4430e+02,  1.2204e+03, -1.6860e+03,\n",
       "          -8.3835e+02, -6.5034e+02,  9.5285e+02,  2.3980e+02, -1.1831e+03,\n",
       "           2.6117e+03,  1.0546e+03,  1.9107e+03, -2.3264e+03,  1.8161e+03,\n",
       "          -7.4079e+02, -1.7436e+02, -3.9436e+03, -1.5818e+03,  2.6284e+03,\n",
       "          -1.3859e+03,  1.2114e+03, -1.4883e+03,  1.6759e+04, -3.2668e+02,\n",
       "           6.3491e+01,  9.8414e+02,  8.7079e+02,  2.1435e+03],\n",
       "         [-2.0805e+03,  1.4447e+02,  2.5358e+01, -1.0085e+03, -1.9370e+03,\n",
       "          -2.0323e+03, -2.0825e+03,  2.4059e+03, -6.8619e+02, -1.3833e+03,\n",
       "           1.9665e+03,  9.4133e+01, -2.7515e+03, -2.5379e+03, -1.7723e+03,\n",
       "           5.7279e+02, -2.6174e+03, -2.6426e+03, -4.4347e+02,  4.9825e+02,\n",
       "          -7.3838e+02, -8.2472e+01,  2.7690e+02, -3.2668e+02,  1.9770e+04,\n",
       "          -4.1521e+02,  2.2505e+03,  2.5224e+03,  1.9595e+02],\n",
       "         [ 7.7382e+02, -6.9577e+02, -1.0452e+03, -6.6735e+01,  2.2522e+03,\n",
       "          -4.7488e+02, -1.9667e+03,  1.4797e+01, -1.5139e+03, -2.0284e+03,\n",
       "          -1.2302e+02, -1.1926e+03,  1.0704e+03,  1.6996e+02,  1.8360e+03,\n",
       "          -6.4991e+02, -8.2590e+02, -1.4723e+02, -3.6250e+03, -5.8775e+02,\n",
       "          -6.2786e+01,  1.0761e+03, -2.1035e+03,  6.3491e+01, -4.1521e+02,\n",
       "           1.4548e+04,  7.7522e+02, -7.8213e+00,  1.1231e+01],\n",
       "         [-2.5679e+03,  3.5558e+02, -1.8335e+03, -2.9244e+02, -1.3631e+03,\n",
       "          -2.2258e+02, -6.3713e+02,  3.2927e+01,  4.3952e+02, -2.3081e+02,\n",
       "           9.3756e+02, -2.5111e+03, -1.3023e+03,  1.4013e+02, -2.1066e+02,\n",
       "          -3.4611e+03, -2.7114e+02,  2.3790e+03,  1.6852e+03,  7.3690e+02,\n",
       "          -1.0046e+03, -1.6858e+03, -7.9128e+02,  9.8414e+02,  2.2505e+03,\n",
       "           7.7522e+02,  1.6780e+04, -1.4965e+02, -4.7275e+02],\n",
       "         [ 1.3081e+03,  1.7925e+03,  2.5613e+03,  3.1726e+02,  2.6044e+03,\n",
       "           8.3697e+01, -1.2485e+03, -1.9296e+03,  4.3524e+02, -8.7431e+02,\n",
       "           1.0513e+03, -5.9279e+02, -1.2551e+03, -6.2864e+02, -6.9618e+02,\n",
       "           3.5934e+02, -2.1364e+03, -2.5793e+03, -2.1582e+03,  8.8308e+02,\n",
       "           8.6265e+02, -1.0053e+03, -2.5361e+02,  8.7079e+02,  2.5224e+03,\n",
       "          -7.8213e+00, -1.4965e+02,  1.9665e+04,  1.3771e+03],\n",
       "         [ 1.0738e+03, -1.6343e+02,  2.1758e+03, -9.2062e+01,  3.7681e+03,\n",
       "          -3.5643e+02,  7.6695e+02,  3.5134e+02,  3.2218e+02, -7.2299e+02,\n",
       "          -1.2597e+03, -2.0614e+03,  7.0670e+02, -2.8938e+02,  1.8985e+03,\n",
       "          -2.2871e+02,  1.2233e+03, -1.7361e+03, -1.0356e+03,  3.0033e+02,\n",
       "          -3.1488e+02,  1.4787e+03, -4.4925e+01,  2.1435e+03,  1.9595e+02,\n",
       "           1.1231e+01, -4.7275e+02,  1.3771e+03,  1.3862e+04]]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.masked_fill(b_mask == 0, -1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = torch.ones(src.size(1), src.size(1)).type_as(src.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1, 1, 0, 1, 1, 1, 1, 1]]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 1, 1, 1]]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_ = src_mask & mat & src_mask.transpose(-2, -1)\n",
    "mat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 1, 1, 1, 1, 1]]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_ & src_mask.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 29, 29])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = torch.matmul(x, x.transpose(-2,-1))\n",
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(src.size(1), src.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_mask = (b != 1).unsqueeze(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_mask = tgt_mask & Variable(model.subsequent_mask(b.size(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 29])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "얘 왜쓰지,,,;;;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src) + 2)\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg, \n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss_compute\n",
    " Label Smoothing Skip\n",
    " - nn.CrossEntropyLoss(*input, *target), where *input = p_ditst, *target = target label in each rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0 # Padding Value들 0으로 설정\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "        \n",
    "    def __call__(self, x, y, norm=1):\n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "        return loss.data * norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CE Loss Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6863,  1.8296,  2.0636,  0.9022,  0.4992],\n",
       "        [-0.9279, -0.8435, -1.1101, -0.6536, -0.1928],\n",
       "        [ 0.0459,  2.0524,  0.2739,  0.0477, -1.3499]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6500, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 128\n",
    "padding_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 30000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29.4953, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(input_[0], b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dist = x.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_dist.fill_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2],\n",
       "        [  194],\n",
       "        [   28],\n",
       "        [  145],\n",
       "        [   15],\n",
       "        [ 1074],\n",
       "        [   59],\n",
       "        [  130],\n",
       "        [   15],\n",
       "        [   26],\n",
       "        [   95],\n",
       "        [    8],\n",
       "        [ 4443],\n",
       "        [   15],\n",
       "        [   26],\n",
       "        [   95],\n",
       "        [   41],\n",
       "        [ 3684],\n",
       "        [ 3486],\n",
       "        [    8],\n",
       "        [  314],\n",
       "        [ 3486],\n",
       "        [    9],\n",
       "        [   66],\n",
       "        [ 1253],\n",
       "        [    4],\n",
       "        [ 1302],\n",
       "        [    5],\n",
       "        [    4],\n",
       "        [ 3907],\n",
       "        [    5],\n",
       "        [  764],\n",
       "        [  916],\n",
       "        [    7],\n",
       "        [20609],\n",
       "        [    5],\n",
       "        [   68],\n",
       "        [  233],\n",
       "        [  345],\n",
       "        [    5],\n",
       "        [    4],\n",
       "        [ 4189],\n",
       "        [ 1413],\n",
       "        [    5],\n",
       "        [ 1920],\n",
       "        [   10],\n",
       "        [  297],\n",
       "        [  102],\n",
       "        [ 4389],\n",
       "        [    3]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 3: Index tensor must either be empty or have same dimensions as output tensor at c:\\n\\pytorch_1559129895673\\work\\aten\\src\\th\\generic/THTensorEvenMoreMath.cpp:531",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-95b7bbe99058>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrue_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 3: Index tensor must either be empty or have same dimensions as output tensor at c:\\n\\pytorch_1559129895673\\work\\aten\\src\\th\\generic/THTensorEvenMoreMath.cpp:531"
     ]
    }
   ],
   "source": [
    "true_dist.scatter_(1, b.unsqueeze(1), .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check with synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import subsequent_mask\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        data[:, 0] = 2 # <s>\n",
    "        data[:, -1] = 3 # <\\s>\n",
    "        src = Variable(data, requires_grad=False).type(torch.long)\n",
    "        tgt = Variable(data, requires_grad=False).type(torch.long)\n",
    "        yield Batch(src, tgt, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.from_numpy(np.random.randint(1, V, size=(30, 10)))\n",
    "Variable(data, requires_grad=False).type(torch.long).type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 11\n",
    "dummy = data_gen(11, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lut = nn.Embedding(11, 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 11\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = trf.Transformer(V, V, N = 2, d_model = 128, d_ff = 512, h=4)\n",
    "model_opt = torch.optim.Adam(model.parameters(), lr = .1, betas=(.9, .98), eps = 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "run_epoch(data_gen(V, 30, 20), model, \n",
    "          SimpleLossCompute(model.generator, criterion, model_opt))\n",
    "model.eval()\n",
    "\n",
    "print(run_epoch(data_gen(V, 30, 20), model, \n",
    "          SimpleLossCompute(model.generator, criterion, None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-06622df31fbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 5, 5, 5, 5, 5, 8, 9, 5, 5]])\n"
     ]
    }
   ],
   "source": [
    "src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]]) )\n",
    "src_mask = Variable(torch.ones(1, 1, 10) )\n",
    "print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol = 2\n",
    "ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory = model.encode(src, src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (40) must match the existing size (10) at non-singleton dimension 3.  Target sizes: [1, 4, 1, 40].  Tensor sizes: [4, 1, 10, 10]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-229-b452c1c86492>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model.decode(memory, src_mask, Variable(ys),\n\u001b[1;32m----> 2\u001b[1;33m             Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n\u001b[0m",
      "\u001b[1;32m~\\Github\\Transformer_Re-Implementation\\model.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, memory, src_mask, tgt, tgt_mask)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtgt_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\colab\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Github\\Transformer_Re-Implementation\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, memory, src_mask, tgt_mask)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\colab\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Github\\Transformer_Re-Implementation\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, memory, src_mask, tgt_mask)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\colab\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Github\\Transformer_Re-Implementation\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, sublayer)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msublayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msublayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Github\\Transformer_Re-Implementation\\model.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msublayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\colab\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Github\\Transformer_Re-Implementation\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, query, key, value, mask)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# 2) 각각에 Attention 적용\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         x, self.attn = attention(query, key, value, mask = mask,\n\u001b[1;32m---> 49\u001b[1;33m                                  dropout = self.dropout)\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# 3) 각 Output 다시 모은 후 Final Linear Projection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Github\\Transformer_Re-Implementation\\model.py\u001b[0m in \u001b[0;36mattention\u001b[1;34m(query, key, value, mask, dropout)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1e9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mp_attn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\colab\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mmasked_fill\u001b[1;34m(self, mask, value)\u001b[0m\n\u001b[0;32m    335\u001b[0m         r\"\"\"Out-of-place version of :meth:`torch.Tensor.masked_fill_`\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (40) must match the existing size (10) at non-singleton dimension 3.  Target sizes: [1, 4, 1, 40].  Tensor sizes: [4, 1, 10, 10]"
     ]
    }
   ],
   "source": [
    "model.decode(memory, src_mask, Variable(ys),\n",
    "            Variable(subsequent_mask(ys.size(1)).type_as(src.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask = Batch.make_std_mask(src, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "         [0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [0, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "         [0, 1, 1, 1, 1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "            \n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebatch(pad_idx, batch):\n",
    "    \"Fix order in torchtext to match ours\"\n",
    "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
    "    return Batch(src, trg, pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =================================\n",
    "### Simple iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process import get_data\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"dataset/\"\n",
    "file_name = \"train.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC, TGT, trn = get_data(data_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = TGT.vocab.stoi[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trf.Transformer(len(SRC.vocab), len(TGT.vocab), N = 6)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
